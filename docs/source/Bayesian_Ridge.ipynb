{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This page provides documentation for the Bayesian Ridge Regression code. The code includes data preprocessing, model training, and visualization. Each section below corresponds to a specific part of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section, we define various parameters and flags used throughout the code. These parameters include file paths, preprocessing options, encoding methods, and hyperparameters for the Bayesian Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the training/handling of the data and model\n",
    "flags.DEFINE_string('data_path', './data', 'Path to the data')\n",
    "flags.DEFINE_string('model_path', './models', 'Path to the model')\n",
    "flags.DEFINE_integer('seed', 123, 'Seed for the random number generator')\n",
    "flags.DEFINE_float('train_size', 0.8, 'the split between train and test dataset')\n",
    "flags.DEFINE_enum('preprocess', None, ['normalize', 'standardize'], 'Preprocessing method')\n",
    "flags.DEFINE_enum('encode', 'one_hot', ['one_hot', 'label'], 'Encoding method')\n",
    "flags.DEFINE_bool('visualize', False, 'enable visualization of the data')\n",
    "flags.DEFINE_bool('train', False, 'enable training of the model')\n",
    "flags.DEFINE_integer('output_index', 0, 'Index of the output to train the model on')\n",
    "flags.DEFINE_bool('custom_dataset', False, 'Wheter to use a custom dataset or not')\n",
    "\n",
    "# Hyperparameters for the model\n",
    "flags.DEFINE_integer('n_iter', 300, 'Maximum number of iterations. The algorithm will converge if it reaches this number of iterations.')\n",
    "flags.DEFINE_float('tol', 1e-3, 'Precision of the solution.')\n",
    "flags.DEFINE_float('alpha_1', 1e-6, 'Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.')\n",
    "flags.DEFINE_float('alpha_2', 1e-6, 'Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.')\n",
    "flags.DEFINE_float('lambda_1', 1e-6, 'Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.')\n",
    "flags.DEFINE_float('lambda_2', 1e-6, 'Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.')\n",
    "flags.DEFINE_float('alpha_init', None, 'Initial value for alpha (precision of the noise). If not set, alpha_init is 1/Var(y).')\n",
    "flags.DEFINE_float('lambda_init', None, 'Initial value for lambda (precision of the weights). If not set, lambda_init is 1.')\n",
    "flags.DEFINE_bool('compute_score', False, 'If True, compute the objective function at each step of the model. Useful for plotting the evolution of the solution.')\n",
    "flags.DEFINE_bool('fit_intercept', True, 'Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).')\n",
    "flags.DEFINE_bool('copy_X', True, 'If True, X will be copied; else, it may be overwritten.')\n",
    "flags.DEFINE_bool('verbose', False, 'Verbose mode when fitting the model.')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The preprocess_data function handles data preprocessing tasks, including encoding categorical features and normalizing numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(actions, observations, exp_path):\n",
    "    observations = observations.to_frame()\n",
    "    # Categorical features\n",
    "    categorical_cols = list(set(actions.columns) - set(actions._get_numeric_data().columns))\n",
    "    if len(categorical_cols) == 0:\n",
    "        NO_CAT = True\n",
    "    else:\n",
    "        NO_CAT = False\n",
    "    categorical_actions = actions[categorical_cols]\n",
    "    \n",
    "    # Numerical features\n",
    "    numerical_actions = actions._get_numeric_data()\n",
    "    \n",
    "    encoder_path = os.path.join(exp_path, 'encoder')\n",
    "    if not os.path.exists(encoder_path):\n",
    "        os.makedirs(encoder_path)\n",
    "\n",
    "    # Encode categorical features\n",
    "    if FLAGS.encode == 'one_hot' and not NO_CAT:\n",
    "        # One-hot encode categorical features\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(categorical_actions)\n",
    "        # Save the encoder\n",
    "        path = os.path.join(encoder_path, 'one_hot_encoder.joblib')\n",
    "        pickle.dump(enc, open(path, 'wb'))\n",
    "        # Transform the categorical features\n",
    "        dummy_col_names = pd.get_dummies(categorical_actions).columns\n",
    "        categorical_actions = pd.DataFrame(enc.transform(categorical_actions).toarray(), columns=dummy_col_names)\n",
    "    elif FLAGS.encode == 'label' and not NO_CAT:\n",
    "        dummy_actions = pd.DataFrame()\n",
    "        for categorical_col in categorical_cols:\n",
    "            # Label encode categorical features\n",
    "            enc = LabelEncoder()\n",
    "            enc.fit(categorical_actions[categorical_col])\n",
    "            # Save the encoder\n",
    "            path = os.path.join(encoder_path, 'label_encoder_{}.joblib'.format(categorical_col))\n",
    "            pickle.dump(enc, open(path, 'wb'))\n",
    "            # Transform the categorical features\n",
    "            dummy_actions[categorical_col] = enc.transform(categorical_actions[categorical_col])\n",
    "        categorical_actions = pd.DataFrame(dummy_actions, columns=categorical_cols)\n",
    "    elif not NO_CAT:\n",
    "        raise ValueError('Encoding method not supported')\n",
    "\n",
    "    preprocess_data_path = os.path.join(exp_path, 'preprocess_data')\n",
    "    if not os.path.exists(preprocess_data_path):\n",
    "        os.makedirs(preprocess_data_path)\n",
    "\n",
    "    # Normalize numerical features\n",
    "    if FLAGS.preprocess is None:\n",
    "        pass\n",
    "    elif FLAGS.preprocess == 'normalize':\n",
    "        # Normalize numerical features for actions\n",
    "        normalize_feature_transformer = MinMaxScaler(feature_range=(0, 1))\n",
    "        normalized_numerical_features = normalize_feature_transformer.fit_transform(numerical_actions)\n",
    "        numerical_actions = pd.DataFrame(normalized_numerical_features, columns=[numerical_actions.columns])\n",
    "        # Save the scaler\n",
    "        path = os.path.join(preprocess_data_path, 'normalize_feature_transformer_actions.joblib')\n",
    "        pickle.dump(normalize_feature_transformer, open(path, 'wb'))\n",
    "        \n",
    "        # Normalize numerical features for observations\n",
    "        normalize_feature_transformer = MinMaxScaler(feature_range=(0, 1))\n",
    "        normalized_numerical_features = normalize_feature_transformer.fit_transform(observations)\n",
    "        observations = pd.DataFrame(normalized_numerical_features, columns=[observations.columns])\n",
    "        # Save the scaler\n",
    "        path = os.path.join(preprocess_data_path, 'normalize_feature_transformer_observations_{}.joblib'.format(FLAGS.output_index))\n",
    "        pickle.dump(normalize_feature_transformer, open(path, 'wb'))\n",
    "    elif FLAGS.preprocess == 'standardize':\n",
    "        # Standardize numerical features for actions\n",
    "        standardize_feature_transformer = StandardScaler()\n",
    "        standardized_numerical_features = standardize_feature_transformer.fit_transform(numerical_actions)\n",
    "        numerical_actions = pd.DataFrame(standardized_numerical_features, columns=[numerical_actions.columns])\n",
    "        # Save the scaler\n",
    "        path = os.path.join(preprocess_data_path, 'standardize_feature_transformer_actions.joblib')\n",
    "        pickle.dump(standardize_feature_transformer, open(path, 'wb'))\n",
    "        \n",
    "        # Standardize numerical features for observations\n",
    "        standardize_feature_transformer = StandardScaler()\n",
    "        standardized_numerical_features = standardize_feature_transformer.fit_transform(observations)\n",
    "        observations = pd.DataFrame(standardized_numerical_features, columns=[observations.columns])\n",
    "        # Save the scaler\n",
    "        path = os.path.join(preprocess_data_path, 'standardize_feature_transformer_observations_{}.joblib'.format(FLAGS.output_index))\n",
    "        pickle.dump(standardize_feature_transformer, open(path, 'wb'))\n",
    "    else:\n",
    "        raise ValueError('Preprocessing method not supported')\n",
    "\n",
    "    # Concatenate numerical and categorical features\n",
    "    if NO_CAT:\n",
    "        actions = numerical_actions.to_numpy()\n",
    "    else:\n",
    "        actions = pd.concat([numerical_actions, categorical_actions], axis = 1).to_numpy()\n",
    "    observations = observations.to_numpy()\n",
    "\n",
    "    return actions, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The visualize_data function visualizes the data distribution and performs a Box-Cox transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data, exp_path):\n",
    "    visualize_path = os.path.join(exp_path, 'visualize')\n",
    "    if not os.path.exists(visualize_path):\n",
    "        os.makedirs(visualize_path)\n",
    "\n",
    "    fig, ax = plt.subplots(data.shape[1], 2)\n",
    "    \n",
    "    lambda_values = []\n",
    "    for i in range(data.shape[1]):\n",
    "        sns.distplot(data, hist=True, kde=True, kde_kws={'shade': True, 'linewidth': 2},\n",
    "                 label='Non-Normal', color='green', ax=ax[i,0])\n",
    "    \n",
    "        fitted_data, fitted_lambda = stats.boxcox(data.iloc[:,i])\n",
    "        lambda_values.append(fitted_lambda)\n",
    "\n",
    "        sns.distplot(fitted_data, hist=True, kde=True, kde_kws={'shade': True, 'linewidth': 2},\n",
    "                 label='Non-Normal', color='green', ax=ax[i,1])\n",
    "    \n",
    "\n",
    "    f = open(os.path.join(visualize_path, 'data_visualization.txt'), 'w')\n",
    "\n",
    "    for i in range(len(lambda_values)):\n",
    "        print('Lambda value used for Transformation in {} Sample {}'.format(list(data.columns)[i], lambda_values[i]))\n",
    "        f.write('Lambda value used for Transformation in {} Sample {}\\n'.format(list(data.columns)[i], lambda_values[i]))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(15)\n",
    "    # Save the figure\n",
    "    fig.savefig(os.path.join(visualize_path, 'data_visualization.png'))\n",
    "    # Show the figure autoclose after 5 seconds\n",
    "    plt.show(block=False)\n",
    "    plt.pause(5)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the main function, data is loaded either from a custom dataset or the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "    if FLAGS.custom_dataset:\n",
    "        actions_path = os.path.join(FLAGS.data_path, 'actions_feasible.csv')\n",
    "        observations_path = os.path.join(FLAGS.data_path, 'observations_feasible.csv')\n",
    "        actions = pd.read_csv(actions_path)\n",
    "        observations = pd.read_csv(observations_path)\n",
    "    else:\n",
    "        california = fetch_california_housing()\n",
    "        actions = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "        observations = pd.DataFrame(california.target, columns=['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing is performed using the preprocess_data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_data(actions, output, exp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional data visualization can be enabled by setting the visualize flag to 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "if FLAGS.visualize:\n",
    "    visualize_data(observations, exp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the train flag is set to True, the Bayesian Ridge regression model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.train:\n",
    "        print('------Training the model------')\n",
    "        # Split the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=FLAGS.train_size, random_state=FLAGS.seed)\n",
    "\n",
    "        # Define the model\n",
    "        regressor = BayesianRidge(n_iter=FLAGS.n_iter, tol=FLAGS.tol, alpha_1=FLAGS.alpha_1, alpha_2=FLAGS.alpha_2,\n",
    "                                  lambda_1=FLAGS.lambda_1, lambda_2=FLAGS.lambda_2, alpha_init=FLAGS.alpha_init, \n",
    "                                  lambda_init=FLAGS.lambda_init, compute_score=FLAGS.compute_score, \n",
    "                                  fit_intercept=FLAGS.fit_intercept, copy_X=FLAGS.copy_X, verbose=FLAGS.verbose)\n",
    "        \n",
    "        # Train the model\n",
    "        regressor.fit(X_train, y_train[:, 0])\n",
    "\n",
    "        # Evaluate the model for train dataset\n",
    "        y_pred = regressor.predict(X_train)\n",
    "        mse_train = mse(y_train, y_pred)\n",
    "        print('MSE on train set: {}'.format(mse_train))\n",
    "\n",
    "        # Evaluate the model for test dataset\n",
    "        y_pred = regressor.predict(X_test)\n",
    "        mse_test = mse(y_test, y_pred)\n",
    "        print('MSE on test set: {}'.format(mse_test))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
