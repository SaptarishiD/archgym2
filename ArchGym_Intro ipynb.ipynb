{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Architecture Gymnasium Tutorial*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0UeBvuxiRVBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Vizier ArchGym Integration</h3>"
      ],
      "metadata": {
        "id": "U5PRpbggsybf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Custom Gym Environment"
      ],
      "metadata": {
        "id": "UDcZRFjwtuD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "from absl import flags\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CustomEnv, self).__init__()\n",
        "        self.observation_space = spaces.Dict({\"energy\": spaces.Box(0, 1, (1,)),\n",
        "                                              \"area\": spaces.Box(0, 1, (1,)),\n",
        "                                              \"latency\": spaces.Box(0, 1, (1,))})\n",
        "\n",
        "        self.action_space = spaces.Dict({\"num_cores\": spaces.Discrete(15),\n",
        "                                         \"freq\": spaces.Box(low = 0.5, high = 3, dtype = float),\n",
        "                                        \"mem_type\": spaces.Discrete(3), # mem_type is one of 'DRAM', 'SRAM', 'Hybrid'\n",
        "                                        \"mem_size\": spaces.Discrete(65)})\n",
        "\n",
        "\n",
        "        self.energy = 0\n",
        "        self.area = 0\n",
        "        self.latency = 0\n",
        "\n",
        "        self.initial_state = np.array([self.energy, self.area, self.latency])\n",
        "        self.ideal = np.array([4, 2.0, 1, 32]) #ideal values for action space [num_cores, freq, mem_type, mem_size]\n",
        "\n",
        "    def reset(self):\n",
        "        return self.initial_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Step function for the environment\n",
        "        \"\"\"\n",
        "        num_cores = action['num_cores']\n",
        "        freq = action['freq']\n",
        "        mem_type = action['mem_type']\n",
        "        mem_size = action['mem_size']\n",
        "\n",
        "        action = np.array([num_cores, freq, mem_type, mem_size])\n",
        "\n",
        "        # Compute the new state based on the action (random formulae for now)\n",
        "        self.energy += num_cores*1 + freq*2 + mem_size*3\n",
        "        self.area += num_cores*2 + freq*3 + mem_size*1\n",
        "        self.latency += num_cores*3 + freq*3 + mem_size*1\n",
        "\n",
        "        observation = np.array([self.energy, self.area, self.latency])\n",
        "\n",
        "        reward = -np.linalg.norm(action - self.ideal)\n",
        "\n",
        "        done = True\n",
        "\n",
        "        return observation, reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print (f'Energy: {self.energy}, Area: {self.area}, Latency: {self.latency}')\n",
        ""
      ],
      "metadata": {
        "id": "bHYZyz9DtgQ4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Environment Wrapper - Wraps an OpenAI Gym environment to be used as a dm_env environment"
      ],
      "metadata": {
        "id": "6npK1jrOt0t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2018 DeepMind Technologies Limited. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Wraps an OpenAI Gym environment to be used as a dm_env environment.\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from acme import specs\n",
        "from acme import types\n",
        "from acme import wrappers\n",
        "import dm_env\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import tree\n",
        "\n",
        "\n",
        "# print(os.sys.path)\n",
        "# from configs import arch_gym_configs\n",
        "# from arch_gym.envs.custom_env import CustomEnv\n",
        "# from envHelpers import helpers\n",
        "class CustomEnvWrapper(dm_env.Environment):\n",
        "  \"\"\"Environment wrapper for OpenAI Gym environments.\"\"\"\n",
        "\n",
        "  # Note: we don't inherit from base.EnvironmentWrapper because that class\n",
        "  # assumes that the wrapped environment is a dm_env.Environment.\n",
        "\n",
        "  def __init__(self, environment: gym.Env):\n",
        "\n",
        "    self._environment = environment\n",
        "    self._reset_next_step = True\n",
        "    self._last_info = None\n",
        "    # self.helper = helpers()\n",
        "\n",
        "    # Convert action and observation specs.\n",
        "    obs_space = self._environment.observation_space\n",
        "    act_space = self._environment.action_space\n",
        "    self._observation_spec = _convert_to_spec(obs_space, name='observation')\n",
        "    self._action_spec = _convert_to_spec(act_space, name='action')\n",
        "    print(\"wrapper\", environment.observation)\n",
        "\n",
        "  def reset(self) -> dm_env.TimeStep:\n",
        "    \"\"\"Resets the episode.\"\"\"\n",
        "    self._reset_next_step = False\n",
        "    observation = self._environment.reset()\n",
        "    print(\"-----Wrapper observation-----\", observation)\n",
        "    # Reset the diagnostic information.\n",
        "    self._last_info = None\n",
        "    a = dm_env.restart(observation)\n",
        "    print(\"dm_env.restart(observation)\",a)\n",
        "    return a\n",
        "\n",
        "  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n",
        "    \"\"\"Steps the environment.\"\"\"\n",
        "    if self._reset_next_step:\n",
        "      return self.reset()\n",
        "\n",
        "    observation, reward, done, info = self._environment.step(action)\n",
        "    print(\"wrapper step\", observation)\n",
        "    print(\"wrapper step rew\", reward)\n",
        "    self._reset_next_step = done\n",
        "    self._last_info = info\n",
        "\n",
        "    # Convert the type of the reward based on the spec, respecting the scalar or\n",
        "    # array property.\n",
        "    reward = tree.map_structure(\n",
        "        lambda x, t: (  # pylint: disable=g-long-lambda\n",
        "            t.dtype.type(x)\n",
        "            if np.isscalar(x) else np.asarray(x, dtype=t.dtype)),\n",
        "        reward,\n",
        "        self.reward_spec())\n",
        "\n",
        "    if done:\n",
        "      truncated = info.get('TimeLimit.truncated', False)\n",
        "      if truncated:\n",
        "        return dm_env.truncation(reward, observation)\n",
        "      return dm_env.termination(reward, observation)\n",
        "    return dm_env.transition(reward, observation)\n",
        "\n",
        "  def observation_spec(self) -> types.NestedSpec:\n",
        "    return self._observation_spec\n",
        "\n",
        "  def action_spec(self) -> types.NestedSpec:\n",
        "    return self._action_spec\n",
        "\n",
        "  def reward_spec(self):\n",
        "    return specs.Array(shape=(), dtype=float, name='reward')\n",
        "\n",
        "  def get_info(self) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Returns the last info returned from env.step(action).\n",
        "    Returns:\n",
        "      info: dictionary of diagnostic information from the last environment step\n",
        "    \"\"\"\n",
        "    return self._last_info\n",
        "\n",
        "  @property\n",
        "  def environment(self) -> gym.Env:\n",
        "    \"\"\"Returns the wrapped environment.\"\"\"\n",
        "    return self._environment\n",
        "\n",
        "  def __getattr__(self, name: str):\n",
        "    if name.startswith('__'):\n",
        "      raise AttributeError(\n",
        "          \"attempted to get missing private attribute '{}'\".format(name))\n",
        "    return getattr(self._environment, name)\n",
        "\n",
        "  def close(self):\n",
        "    self._environment.close()\n",
        "\n",
        "\n",
        "def _convert_to_spec(space: gym.Space,\n",
        "                     name: Optional[str] = None) -> types.NestedSpec:\n",
        "  \"\"\"Converts an OpenAI Gym space to a dm_env spec or nested structure of specs.\n",
        "  Box, MultiBinary and MultiDiscrete Gym spaces are converted to BoundedArray\n",
        "  specs. Discrete OpenAI spaces are converted to DiscreteArray specs. Tuple and\n",
        "  Dict spaces are recursively converted to tuples and dictionaries of specs.\n",
        "  Args:\n",
        "    space: The Gym space to convert.\n",
        "    name: Optional name to apply to all return spec(s).\n",
        "  Returns:\n",
        "    A dm_env spec or nested structure of specs, corresponding to the input\n",
        "    space.\n",
        "  \"\"\"\n",
        "  if isinstance(space, spaces.Discrete):\n",
        "    return specs.DiscreteArray(num_values=space.n, dtype=space.dtype, name=name)\n",
        "\n",
        "  elif isinstance(space, spaces.Box):\n",
        "    return specs.BoundedArray(\n",
        "        shape=space.shape,\n",
        "        dtype=space.dtype,\n",
        "        minimum=space.low,\n",
        "        maximum=space.high,\n",
        "        name=name)\n",
        "\n",
        "  elif isinstance(space, spaces.MultiBinary):\n",
        "    return specs.BoundedArray(\n",
        "        shape=space.shape,\n",
        "        dtype=space.dtype,\n",
        "        minimum=0.0,\n",
        "        maximum=1.0,\n",
        "        name=name)\n",
        "\n",
        "  elif isinstance(space, spaces.MultiDiscrete):\n",
        "    return specs.BoundedArray(\n",
        "        shape=space.shape,\n",
        "        dtype=space.dtype,\n",
        "        minimum=np.zeros(space.shape),\n",
        "        maximum=space.nvec - 1,\n",
        "        name=name)\n",
        "\n",
        "  elif isinstance(space, spaces.Tuple):\n",
        "    return tuple(_convert_to_spec(s, name) for s in space.spaces)\n",
        "\n",
        "  elif isinstance(space, spaces.Dict):\n",
        "    return {\n",
        "        key: _convert_to_spec(value, key)\n",
        "        for key, value in space.spaces.items()\n",
        "    }\n",
        "\n",
        "  else:\n",
        "    raise ValueError('Unexpected gym space: {}'.format(space))\n",
        "\n",
        "def make_custom_env(seed: int = 12234,\n",
        "                     max_steps: int = 100,\n",
        "                     reward_formulation: str = 'power',\n",
        "                     ) -> dm_env.Environment:\n",
        "  \"\"\"Returns DRAMSys environment.\"\"\"\n",
        "  environment = CustomEnvWrapper(CustomEnv(max_steps=max_steps))\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "#   if(arch_gym_configs.rl_agent):\n",
        "#     environment = wrappers.CanonicalSpecWrapper(environment, clip=True)\n",
        "  return environment"
      ],
      "metadata": {
        "id": "URP2J7scs_3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code: Random Search Vizier Algorithm for Design Space exploration in custom environment"
      ],
      "metadata": {
        "id": "QMfz8x8JufqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent import futures\n",
        "import grpc\n",
        "import portpicker\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "from absl import flags\n",
        "from absl import app\n",
        "from absl import logging\n",
        "\n",
        "# os.sys.path.insert(0, os.path.abspath('../../'))\n",
        "# from configs import arch_gym_configs\n",
        "# from arch_gym.envs.envHelpers import helpers\n",
        "\n",
        "import envlogger\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from vizier._src.algorithms.designers.random import RandomDesigner\n",
        "# from arch_gym.envs import customenv_wrapper\n",
        "from vizier.service import clients\n",
        "from vizier.service import pyvizier as vz\n",
        "from vizier.service import vizier_server\n",
        "from vizier.service import vizier_service_pb2_grpc\n",
        "\n",
        "flags.DEFINE_string('workload', 'stream.stl', 'Which DRAMSys workload to run?')\n",
        "flags.DEFINE_integer('num_steps', 100, 'Number of training steps.')\n",
        "flags.DEFINE_integer('num_episodes', 2, 'Number of training episodes.')\n",
        "flags.DEFINE_string('traject_dir',\n",
        "                    'random_search_trajectories',\n",
        "            'Directory to save the dataset.')\n",
        "flags.DEFINE_bool('use_envlogger', False, 'Use envlogger to log the data.')\n",
        "flags.DEFINE_string('summary_dir', '.', 'Directory to save the summary.')\n",
        "flags.DEFINE_string('reward_formulation', 'power', 'Which reward formulation to use?')\n",
        "flags.DEFINE_integer('seed', 110, 'random_search_hyperparameter')\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "def log_fitness_to_csv(filename, fitness_dict):\n",
        "    \"\"\"Logs fitness history to csv file\n",
        "\n",
        "    Args:\n",
        "        filename (str): path to the csv file\n",
        "        fitness_dict (dict): dictionary containing the fitness history\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame([fitness_dict['reward']])\n",
        "    csvfile = os.path.join(filename, \"fitness.csv\")\n",
        "    df.to_csv(csvfile, index=False, header=False, mode='a')\n",
        "\n",
        "    # append to csv\n",
        "    df = pd.DataFrame([fitness_dict])\n",
        "    csvfile = os.path.join(filename, \"trajectory.csv\")\n",
        "    df.to_csv(csvfile, index=False, header=False, mode='a')\n",
        "\n",
        "def wrap_in_envlogger(env, envlogger_dir):\n",
        "    \"\"\"Wraps the environment in envlogger\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): gym environment\n",
        "        envlogger_dir (str): path to the directory where the data will be logged\n",
        "    \"\"\"\n",
        "    metadata = {\n",
        "        'agent_type': 'RandomSearch',\n",
        "        'num_steps': FLAGS.num_steps,\n",
        "        'env_type': type(env).__name__,\n",
        "    }\n",
        "    if FLAGS.use_envlogger:\n",
        "        logging.info('Wrapping environment with EnvironmentLogger...')\n",
        "        env = envlogger.EnvLogger(env,\n",
        "                                  data_directory=envlogger_dir,\n",
        "                                  max_episodes_per_file=1000,\n",
        "                                  metadata=metadata)\n",
        "        logging.info('Done wrapping environment with EnvironmentLogger.')\n",
        "        return env\n",
        "    else:\n",
        "        return env\n",
        "\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    \"\"\"Trains the custom environment using random actions for a given number of steps and episodes\n",
        "    \"\"\"\n",
        "\n",
        "    env = customenv_wrapper.make_custom_env(max_steps=FLAGS.num_steps)\n",
        "    fitness_hist = {}\n",
        "    problem = vz.ProblemStatement()\n",
        "    problem.search_space.select_root().add_int_param(name='num_cores', min_value = 1, max_value = 12)\n",
        "    problem.search_space.select_root().add_float_param(name='freq', min_value = 0.5, max_value = 3)\n",
        "    problem.search_space.select_root().add_categorical_param(name='mem_type', feasible_values =['DRAM', 'SRAM', 'Hybrid'])\n",
        "    problem.search_space.select_root().add_discrete_param(name='mem_size', feasible_values=[0, 32, 64, 128, 256, 512])\n",
        "\n",
        "    problem.metric_information.append(\n",
        "        vz.MetricInformation(\n",
        "            name='Reward', goal=vz.ObjectiveMetricGoal.MAXIMIZE))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    study_config = vz.StudyConfig.from_problem(problem)\n",
        "    # study_config.algorithm = vz.Algorithm.RANDOM_SEARCH\n",
        "    random_designer = RandomDesigner(problem.search_space, seed = FLAGS.seed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    port = portpicker.pick_unused_port()\n",
        "    address = f'localhost:{port}'\n",
        "\n",
        "    # Setup server.\n",
        "    server = grpc.server(futures.ThreadPoolExecutor(max_workers=100))\n",
        "\n",
        "    # Setup Vizier Service.\n",
        "    servicer = vizier_server.VizierService()\n",
        "    vizier_service_pb2_grpc.add_VizierServiceServicer_to_server(servicer, server)\n",
        "    server.add_secure_port(address, grpc.local_server_credentials())\n",
        "\n",
        "    # Start the server.\n",
        "    server.start()\n",
        "\n",
        "    clients.environment_variables.service_endpoint = address  # Server address.\n",
        "    study = clients.Study.from_study_config(\n",
        "        study_config, owner='owner', study_id='example_study_id')\n",
        "\n",
        "     # experiment name\n",
        "    exp_name = \"_num_steps_\" + str(FLAGS.num_steps) + \"_num_episodes_\" + str(FLAGS.num_episodes)\n",
        "\n",
        "    # append logs to base path\n",
        "    log_path = os.path.join(FLAGS.summary_dir, 'random_search_logs', FLAGS.reward_formulation, exp_name)\n",
        "\n",
        "    # get the current working directory and append the exp name\n",
        "    traject_dir = os.path.join(FLAGS.summary_dir, FLAGS.traject_dir, FLAGS.reward_formulation, exp_name)\n",
        "\n",
        "    # check if log_path exists else create it\n",
        "    if not os.path.exists(log_path):\n",
        "        os.makedirs(log_path)\n",
        "\n",
        "    if FLAGS.use_envlogger:\n",
        "        if not os.path.exists(traject_dir):\n",
        "            os.makedirs(traject_dir)\n",
        "    env = wrap_in_envlogger(env, traject_dir)\n",
        "\n",
        "    # \"\"\"\n",
        "    # This loop runs for num_steps * num_episodes iterations.\n",
        "    # \"\"\"\n",
        "    env.reset()\n",
        "\n",
        "    count = 0\n",
        "    suggestions = random_designer.suggest(count=FLAGS.num_steps)\n",
        "\n",
        "    for suggestion in suggestions:\n",
        "        count += 1\n",
        "        num_cores = str(suggestion.parameters['num_cores'])\n",
        "        freq = str(suggestion.parameters['freq'])\n",
        "        mem_type_dict = {'DRAM':0, 'SRAM':1, 'Hybrid':2}\n",
        "        mem_type = str(mem_type_dict[str(suggestion.parameters['mem_type'])])\n",
        "        mem_size = str(suggestion.parameters['mem_size'])\n",
        "\n",
        "        action = {\"num_cores\":float(num_cores), \"freq\": float(freq), \"mem_type\":float(mem_type), \"mem_size\": float(mem_size)}\n",
        "\n",
        "        print(\"Suggested Parameters for num_cores, freq, mem_type, mem_size are :\", num_cores, freq, mem_type, mem_size)\n",
        "        done, reward, info, obs = (env.step(action))\n",
        "        fitness_hist['reward'] = reward\n",
        "        fitness_hist['action'] = action\n",
        "        fitness_hist['obs'] = obs\n",
        "        if count == FLAGS.num_steps:\n",
        "            done = True\n",
        "\n",
        "        log_fitness_to_csv(log_path, fitness_hist)\n",
        "        print(\"Observation: \",obs)\n",
        "        final_measurement = vz.Measurement({'Reward': reward})\n",
        "        suggestion = suggestion.to_trial()\n",
        "        suggestion.complete(final_measurement)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   app.run(main)"
      ],
      "metadata": {
        "id": "Vinsqcx8uesW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}